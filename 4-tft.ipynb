{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x7bMSWYPWfWG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "🖥️ Device: mps | File: nasdaq100_stock_data.csv | Tickers: 101\n",
      "\n",
      "[1/101] 🎯 FANG\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 475\u001b[39m\n\u001b[32m    472\u001b[39m model = TFTRegressor(input_size=\u001b[38;5;28mlen\u001b[39m(feat_cols), hidden_size=hidden_size,\n\u001b[32m    473\u001b[39m                      num_layers=num_layers, num_heads=num_heads, dropout=dropout)\n\u001b[32m    474\u001b[39m trainer = Trainer(model, device=device, autocast_enabled=autocast_enabled)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3e-4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[38;5;66;03m# Carga mejor checkpoint y predice\u001b[39;00m\n\u001b[32m    478\u001b[39m model.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mbest_tft_return.pth\u001b[39m\u001b[33m\"\u001b[39m, map_location=device))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 334\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, train_loader, val_loader, epochs, lr)\u001b[39m\n\u001b[32m    331\u001b[39m best = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33minf\u001b[39m\u001b[33m'\u001b[39m); patience=\u001b[32m15\u001b[39m; wait=\u001b[32m0\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs+\u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     tr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m; sch.step()\n\u001b[32m    335\u001b[39m     va = \u001b[38;5;28mself\u001b[39m._run_epoch(val_loader, train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    336\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_losses.append(tr); \u001b[38;5;28mself\u001b[39m.val_losses.append(va)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 322\u001b[39m, in \u001b[36mTrainer._run_epoch\u001b[39m\u001b[34m(self, loader, opt, train)\u001b[39m\n\u001b[32m    320\u001b[39m             loss.backward()\n\u001b[32m    321\u001b[39m             torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), \u001b[32m0.8\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m             \u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m         total += loss.item()\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total/\u001b[38;5;28mlen\u001b[39m(loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-BehavioralPensions/GDA/gdiazamor.ds/CS_DD/Master/2023_24/TFM/.venv/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:133\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m opt = opt_ref()\n\u001b[32m    132\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-BehavioralPensions/GDA/gdiazamor.ds/CS_DD/Master/2023_24/TFM/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:516\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    511\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    512\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    513\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-BehavioralPensions/GDA/gdiazamor.ds/CS_DD/Master/2023_24/TFM/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-BehavioralPensions/GDA/gdiazamor.ds/CS_DD/Master/2023_24/TFM/.venv/lib/python3.13/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-BehavioralPensions/GDA/gdiazamor.ds/CS_DD/Master/2023_24/TFM/.venv/lib/python3.13/site-packages/torch/optim/optimizer.py:149\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-BehavioralPensions/GDA/gdiazamor.ds/CS_DD/Master/2023_24/TFM/.venv/lib/python3.13/site-packages/torch/optim/adam.py:949\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    946\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    947\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m949\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-BehavioralPensions/GDA/gdiazamor.ds/CS_DD/Master/2023_24/TFM/.venv/lib/python3.13/site-packages/torch/optim/adam.py:411\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[32m    410\u001b[39m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m         \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    413\u001b[39m         \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    414\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight_decay, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Fast TFT — Regression on next return (MPS‑ready, macOS-safe DataLoader)\n",
    "# =========================================\n",
    "import os, math, warnings, random\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------------\n",
    "# Repro\n",
    "# -----------------------------\n",
    "def set_seed(seed=1337):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "set_seed(1337)\n",
    "\n",
    "# Mejora en MPS\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# Feature engineering (TFT full)\n",
    "# -----------------------------\n",
    "def add_calendar_features(df):\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"DoW\"] = df[\"Date\"].dt.dayofweek\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    df[\"Quarter\"] = df[\"Date\"].dt.quarter\n",
    "    df[\"Day\"] = df[\"Date\"].dt.day\n",
    "    # Cyclical encodings\n",
    "    df[\"DoW_sin\"] = np.sin(2*np.pi*df[\"DoW\"]/7);    df[\"DoW_cos\"] = np.cos(2*np.pi*df[\"DoW\"]/7)\n",
    "    df[\"Mon_sin\"] = np.sin(2*np.pi*df[\"Month\"]/12); df[\"Mon_cos\"] = np.cos(2*np.pi*df[\"Month\"]/12)\n",
    "    df[\"Q_sin\"]   = np.sin(2*np.pi*df[\"Quarter\"]/4);df[\"Q_cos\"]  = np.cos(2*np.pi*df[\"Quarter\"]/4)\n",
    "    df[\"DOM_sin\"] = np.sin(2*np.pi*df[\"Day\"]/31);   df[\"DOM_cos\"] = np.cos(2*np.pi*df[\"Day\"]/31)\n",
    "    return df\n",
    "\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Base returns / vol\n",
    "    df[\"Returns\"] = df[\"Close\"].pct_change()\n",
    "    df[\"Volatility\"] = df[\"Returns\"].expanding(min_periods=20).std()\n",
    "\n",
    "    # RSI (14,21)\n",
    "    for period in [14, 21]:\n",
    "        delta = df[\"Close\"].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(period).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(period).mean()\n",
    "        rs = gain / (loss.replace(0, np.nan))\n",
    "        df[f\"RSI_{period}\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # MACD + signal + histogram\n",
    "    ema12 = df[\"Close\"].ewm(span=12).mean()\n",
    "    ema26 = df[\"Close\"].ewm(span=26).mean()\n",
    "    df[\"MACD\"] = ema12 - ema26\n",
    "    df[\"MACD_Signal\"] = df[\"MACD\"].ewm(span=9).mean()\n",
    "    df[\"MACD_Histogram\"] = df[\"MACD\"] - df[\"MACD_Signal\"]\n",
    "\n",
    "    # Bollinger (20,50): pos & width\n",
    "    for period in [20, 50]:\n",
    "        m = df[\"Close\"].rolling(period).mean()\n",
    "        s = df[\"Close\"].rolling(period).std()\n",
    "        upper, lower = m + 2*s, m - 2*s\n",
    "        df[f\"BollingerPos_{period}\"]   = (df[\"Close\"] - lower) / (upper - lower)\n",
    "        df[f\"BollingerWidth_{period}\"] = (upper - lower) / m\n",
    "\n",
    "    # Volume features\n",
    "    df[\"VolumeMA\"] = df[\"Volume\"].rolling(20).mean()\n",
    "    df[\"VolumeRatio\"] = df[\"Volume\"] / df[\"VolumeMA\"]\n",
    "    df[\"VolumePriceCorr\"] = df[\"Volume\"].rolling(20).corr(df[\"Close\"])\n",
    "\n",
    "    # Price ratios\n",
    "    df[\"HighLowRatio\"] = df[\"High\"] / df[\"Low\"]\n",
    "    df[\"CloseOpenRatio\"] = df[\"Close\"] / df[\"Open\"]\n",
    "    df[\"HighCloseRatio\"] = df[\"High\"] / df[\"Close\"]\n",
    "    df[\"LowCloseRatio\"]  = df[\"Low\"]  / df[\"Close\"]\n",
    "\n",
    "    # Multi-horizon returns\n",
    "    for p in [1,3,5,10,20]:\n",
    "        df[f\"Ret_{p}\"] = df[\"Close\"].pct_change(p)\n",
    "\n",
    "    # ATR (14,21)\n",
    "    hl = df[\"High\"] - df[\"Low\"]\n",
    "    hc = (df[\"High\"] - df[\"Close\"].shift()).abs()\n",
    "    lc = (df[\"Low\"]  - df[\"Close\"].shift()).abs()\n",
    "    tr = np.maximum(hl, np.maximum(hc, lc))\n",
    "    for p in [14,21]:\n",
    "        df[f\"ATR_{p}\"] = tr.rolling(p).mean()\n",
    "\n",
    "    # EMA deviations (retorno relativo al EMA)\n",
    "    for span in [5,12,26,50]:\n",
    "        ema = df[\"Close\"].ewm(span=span, adjust=False).mean()\n",
    "        df[f\"EMAdev_{span}\"] = df[\"Close\"] / ema - 1.0\n",
    "\n",
    "    # Momentum / niveles\n",
    "    df[\"Momentum_10\"] = df[\"Close\"] / df[\"Close\"].shift(10) - 1\n",
    "    df[\"Momentum_20\"] = df[\"Close\"] / df[\"Close\"].shift(20) - 1\n",
    "    df[\"HighMax_20\"]  = df[\"High\"].rolling(20).max()\n",
    "    df[\"LowMin_20\"]   = df[\"Low\"].rolling(20).min()\n",
    "    df[\"PricePosition\"] = (df[\"Close\"] - df[\"LowMin_20\"]) / (df[\"HighMax_20\"] - df[\"LowMin_20\"])\n",
    "\n",
    "    # Short lags for returns\n",
    "    df[\"Ret1\"] = df[\"Returns\"].shift(1)\n",
    "    df[\"Ret5\"] = df[\"Close\"].pct_change(5)\n",
    "\n",
    "    df = add_calendar_features(df)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Windows / Splits (TARGET = next return)\n",
    "# -----------------------------\n",
    "def make_sequences_by_dates(df, lookback=60, horizon=1):\n",
    "    \"\"\"\n",
    "    Objetivo: Return_next = log(C_{t+h}/C_t). Métricas de dirección con diff() sobre retornos.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"Target_Return\"] = np.log(df[\"Close\"].shift(-horizon) / df[\"Close\"])\n",
    "\n",
    "    feature_cols = [\n",
    "        \"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\n",
    "        \"RSI_14\",\"RSI_21\",\"MACD\",\"MACD_Signal\",\"MACD_Histogram\",\n",
    "        \"Volatility\",\"BollingerPos_20\",\"BollingerPos_50\",\"BollingerWidth_20\",\"BollingerWidth_50\",\n",
    "        \"VolumeRatio\",\"VolumePriceCorr\",\"HighLowRatio\",\"CloseOpenRatio\",\"HighCloseRatio\",\"LowCloseRatio\",\n",
    "        \"Ret_1\",\"Ret_3\",\"Ret_5\",\"Ret_10\",\"Ret_20\",\n",
    "        \"ATR_14\",\"ATR_21\",\"EMAdev_5\",\"EMAdev_12\",\"EMAdev_26\",\"EMAdev_50\",\n",
    "        \"Momentum_10\",\"Momentum_20\",\"PricePosition\",\n",
    "        \"DoW_sin\",\"DoW_cos\",\"Mon_sin\",\"Mon_cos\",\"Q_sin\",\"Q_cos\",\"DOM_sin\",\"DOM_cos\",\n",
    "        \"Ret1\",\"Ret5\"\n",
    "    ]\n",
    "    feature_cols = [c for c in feature_cols if c in df.columns]\n",
    "\n",
    "    df = df.dropna(subset=feature_cols + [\"Target_Return\"])\n",
    "\n",
    "    X, y, tgt_dates = [], [], []\n",
    "    V = df[feature_cols].values\n",
    "    yv = df[\"Target_Return\"].values\n",
    "    dates = df[\"Date\"].values\n",
    "\n",
    "    for t in range(lookback-1, len(df)-horizon):\n",
    "        X.append(V[t-(lookback-1):t+1, :])\n",
    "        y.append(float(yv[t]))\n",
    "        tgt_dates.append(pd.to_datetime(dates[t+horizon]))\n",
    "    return np.asarray(X), np.asarray(y), np.array(tgt_dates), feature_cols\n",
    "\n",
    "def split_train_val_test_by_date(\n",
    "    tgt_dates,\n",
    "    start_train=\"2020-01-01\", end_trainval=\"2023-12-31\",\n",
    "    test_start=\"2024-01-01\",  test_end=\"2024-12-31\",\n",
    "    val_ratio_within_train=0.20\n",
    "):\n",
    "    d = pd.to_datetime(tgt_dates)\n",
    "    trv = np.where((d>=pd.Timestamp(start_train))&(d<=pd.Timestamp(end_trainval)))[0]\n",
    "    te  = np.where((d>=pd.Timestamp(test_start)) &(d<=pd.Timestamp(test_end)) )[0]\n",
    "    if len(trv)==0 or len(te)==0: return None, None, None\n",
    "    trv = np.sort(trv)\n",
    "    split = int(len(trv)*(1-val_ratio_within_train))\n",
    "    tr, va = trv[:split], trv[split:]\n",
    "    te = np.sort(te)\n",
    "    if len(tr)==0 or len(va)==0 or len(te)==0: return None, None, None\n",
    "    return tr, va, te\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "class ReturnDataset(Dataset):\n",
    "    def __init__(self, X, y, dates=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.dates = dates\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "def prepare_datasets_for_ticker(df_ticker, lookback=60, horizon=1):\n",
    "    df_feat = engineer_features(df_ticker.copy()).dropna()\n",
    "    X, y, tgt_dates, feat_cols = make_sequences_by_dates(df_feat, lookback, horizon)\n",
    "\n",
    "    split = split_train_val_test_by_date(tgt_dates)\n",
    "    if split is None: return [None]*8\n",
    "    tr, va, te = split\n",
    "\n",
    "    scaler_x = StandardScaler()\n",
    "    Xtr_2d = X[tr].reshape(-1, X.shape[-1])\n",
    "    scaler_x.fit(Xtr_2d)\n",
    "\n",
    "    def transform_windows(Xw):\n",
    "        S = Xw.shape\n",
    "        X2 = Xw.reshape(-1, S[-1])\n",
    "        X2 = scaler_x.transform(X2)\n",
    "        return X2.reshape(S)\n",
    "\n",
    "    Xtr = transform_windows(X[tr]); ytr = y[tr]\n",
    "    Xva = transform_windows(X[va]); yva = y[va]\n",
    "    Xte = transform_windows(X[te]); yte = y[te]\n",
    "\n",
    "    return ReturnDataset(Xtr, ytr), ReturnDataset(Xva, yva), ReturnDataset(Xte, yte, dates=tgt_dates[te]), scaler_x, None, tgt_dates[te], feat_cols, y[te]\n",
    "\n",
    "# -----------------------------\n",
    "# TFT ligero — regresión\n",
    "# -----------------------------\n",
    "class EnhancedVariableSelectionNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nets = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, hidden_size),\n",
    "                nn.ELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.ELU()\n",
    "            ) for _ in range(input_size)\n",
    "        ])\n",
    "        self.selector = nn.Sequential(\n",
    "            nn.Linear(hidden_size*input_size, hidden_size),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,F = x.shape\n",
    "        outs = [net(x[:,:,i:i+1]) for i,net in enumerate(self.nets)]\n",
    "        concat = torch.cat(outs, dim=-1)\n",
    "        w = self.selector(concat.view(B*T, -1)).view(B,T,F)\n",
    "        fused = torch.zeros(B,T,self.hidden_size, device=x.device)\n",
    "        for i,o in enumerate(outs):\n",
    "            fused += w[:,:,i:i+1]*o\n",
    "        return self.drop(fused)\n",
    "\n",
    "class InterpretableMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=6, dropout=0.15):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.h = n_heads; self.dk = d_model // n_heads\n",
    "        self.Wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.Wo = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model); self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q,k,v):\n",
    "        B,T,H = q.shape\n",
    "        Q = self.Wq(q).view(B,T,self.h,self.dk).transpose(1,2)\n",
    "        K = self.Wk(k).view(B,T,self.h,self.dk).transpose(1,2)\n",
    "        V = self.Wv(v).view(B,T,self.h,self.dk).transpose(1,2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.dk)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        ctx = torch.matmul(attn, V).transpose(1,2).contiguous().view(B,T,H)\n",
    "        out = self.norm(q + self.drop(self.Wo(ctx)))\n",
    "        return out\n",
    "\n",
    "class TFTRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=96, num_layers=2, num_heads=6, dropout=0.15):\n",
    "        super().__init__()\n",
    "        self.vsn = EnhancedVariableSelectionNetwork(input_size, hidden_size, dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size//2, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.attn = InterpretableMultiHeadAttention(d_model=hidden_size, n_heads=num_heads, dropout=dropout)\n",
    "        self.grn  = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            nn.GLU(dim=-1), nn.ELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(hidden_size); self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size*3, hidden_size),\n",
    "            nn.ELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vsn(x)\n",
    "        enc,_ = self.lstm(x)\n",
    "        att = self.attn(enc, enc, enc)\n",
    "        x = self.norm1(enc + att)\n",
    "        x = self.norm2(x + self.grn(x))\n",
    "        pooled = torch.cat([x.mean(1), x.max(1)[0], x[:,-1,:]], dim=-1)\n",
    "        return self.head(pooled).squeeze(-1)\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer (Huber + OneCycleLR) con autocast MPS\n",
    "# -----------------------------\n",
    "class Trainer:\n",
    "    def __init__(self, model, device='cpu', autocast_enabled=False):\n",
    "        self.model = model.to(device); self.device = device\n",
    "        self.loss_fn = nn.SmoothL1Loss(beta=0.01)\n",
    "        self.train_losses = []; self.val_losses = []\n",
    "        self.autocast_enabled = autocast_enabled\n",
    "\n",
    "    def _run_epoch(self, loader, opt=None, train=True):\n",
    "        if train: self.model.train()\n",
    "        else:     self.model.eval()\n",
    "        total=0.0\n",
    "        with torch.set_grad_enabled(train):\n",
    "            for xb,yb in loader:\n",
    "                xb = xb.to(self.device); yb = yb.to(self.device)\n",
    "                if train: opt.zero_grad(set_to_none=True)\n",
    "                with torch.autocast(device_type=self.device.type, dtype=torch.float16, enabled=self.autocast_enabled):\n",
    "                    preds = self.model(xb); loss = self.loss_fn(preds, yb)\n",
    "                if train:\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.8)\n",
    "                    opt.step()\n",
    "                total += loss.item()\n",
    "        return total/len(loader)\n",
    "\n",
    "    def fit(self, train_loader, val_loader, epochs=80, lr=3e-4):\n",
    "        opt = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.03, betas=(0.9,0.95))\n",
    "        sch = optim.lr_scheduler.OneCycleLR(opt, max_lr=lr, steps_per_epoch=max(1,len(train_loader)),\n",
    "                                            epochs=epochs, pct_start=0.2, anneal_strategy='cos',\n",
    "                                            div_factor=25.0, final_div_factor=100.0)\n",
    "        best = float('inf'); patience=15; wait=0\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            tr = self._run_epoch(train_loader, opt, train=True); sch.step()\n",
    "            va = self._run_epoch(val_loader, train=False)\n",
    "            self.train_losses.append(tr); self.val_losses.append(va)\n",
    "\n",
    "            if va < best: best, wait = va, 0; torch.save(self.model.state_dict(), \"best_tft_return.pth\")\n",
    "            else: wait += 1\n",
    "\n",
    "            if ep % 10 == 0:\n",
    "                print(f\"Epoch {ep:03d} | Train {tr:.5f} | Val {va:.5f}\")\n",
    "            if wait >= patience and ep >= 40:\n",
    "                print(f\"⏹️ Early stopping at epoch {ep}\"); break\n",
    "\n",
    "    def predict(self, loader):\n",
    "        self.model.eval()\n",
    "        preds=[]; targets=[]\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in loader:\n",
    "                xb = xb.to(self.device)\n",
    "                out = self.model(xb).cpu().numpy()\n",
    "                preds.append(out); targets.append(yb.numpy())\n",
    "        return np.concatenate(preds), np.concatenate(targets)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluación (dirección desde retornos) y plots\n",
    "# -----------------------------\n",
    "def evaluate_direction_from_returns(y_pred, y_true, title=\"Confusion Matrix (returns)\"):\n",
    "    y_pred_bin = (pd.Series(y_pred).diff() >= 0).astype(int).iloc[1:]\n",
    "    y_true_bin = (pd.Series(y_true).diff() >= 0).astype(int).iloc[1:]\n",
    "\n",
    "    acc  = accuracy_score(y_true_bin, y_pred_bin)\n",
    "    prec = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "    rec  = recall_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "    f1   = f1_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred)**2))\n",
    "\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.6f}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Down/0','Up/1'], yticklabels=['Down/0','Up/1'])\n",
    "    plt.title(title); plt.xlabel('Predicted'); plt.ylabel('Actual')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    return acc, prec, rec, f1, rmse\n",
    "\n",
    "def plot_return_curve(dates, preds, targets, ticker):\n",
    "    dt = pd.to_datetime(dates)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(dt, targets, label=\"Actual next return\", linewidth=1.2)\n",
    "    plt.plot(dt, preds,   label=\"Predicted next return\", linewidth=1.2, alpha=0.9)\n",
    "    plt.title(f\"{ticker} — Next-return prediction (test 2024)\")\n",
    "    plt.ylabel(\"Return\"); plt.xlabel(\"Date\"); plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_losses(trainer, ticker):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(trainer.train_losses, label='Train')\n",
    "    plt.plot(trainer.val_losses,   label='Val')\n",
    "    plt.title(f'{ticker} — Loss history'); plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Device setup (MPS > CUDA > CPU)\n",
    "# -----------------------------\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    autocast_enabled = True\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    autocast_enabled = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    autocast_enabled = False\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    lookback_window = 60\n",
    "    forecast_horizon = 1\n",
    "    hidden_size = 192\n",
    "    num_layers = 2\n",
    "    num_heads = 8           # 192 % 8 == 0\n",
    "    dropout = 0.15\n",
    "    epochs = 80\n",
    "    batch_size = 48\n",
    "    max_tickers = 101\n",
    "    min_rows_required = 350\n",
    "\n",
    "    # Carga CSV multi‑índice (formato Kaggle)\n",
    "    try_paths = [\n",
    "        \"/kaggle/input/nasdaq100-stock-data/nasdaq100_stock_data.csv\",\n",
    "        \"nasdaq100_stock_data.csv\"\n",
    "    ]\n",
    "    path = next((p for p in try_paths if os.path.exists(p)), try_paths[-1])\n",
    "    df = pd.read_csv(path, header=[0,1])\n",
    "    df = df.iloc[1:].copy()\n",
    "    cols = df.columns.tolist(); cols[0] = ('Date',''); df.columns = pd.MultiIndex.from_tuples(cols)\n",
    "\n",
    "    tickers = df.columns.get_level_values(0).unique()\n",
    "    tickers = [t for t in tickers if t != 'Date']\n",
    "\n",
    "    print(f\"🖥️ Device: {device} | File: {path} | Tickers: {len(tickers)}\")\n",
    "\n",
    "    # 👇 Evita multiprocessing para no “picklear” clases de __main__ en macOS/py3.13\n",
    "    loader_kwargs_train = dict(batch_size=batch_size, shuffle=True,  num_workers=0, persistent_workers=False)\n",
    "    loader_kwargs_eval  = dict(batch_size=batch_size, shuffle=False, num_workers=0, persistent_workers=False)\n",
    "\n",
    "    # Acumuladores por ticker\n",
    "    accuracies, precisions, recalls, f1_scores, RMSES, tick_list = [], [], [], [], [], []\n",
    "\n",
    "    processed = 0\n",
    "    for t in tickers:\n",
    "        if processed >= max_tickers: break\n",
    "        print(f\"\\n[{processed+1}/{max_tickers}] 🎯 {t}\")\n",
    "        try:\n",
    "            dft = df[t].copy().rename_axis(None, axis=1)\n",
    "            dft[\"Date\"] = pd.to_datetime(df[('Date','')].values, errors='coerce')\n",
    "            dft = dft.dropna(subset=[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"])\n",
    "            dft = dft[[\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]].reset_index(drop=True)\n",
    "            if len(dft) < min_rows_required:\n",
    "                print(f\"⚠️  Insufficient rows after cleaning: {len(dft)}\"); continue\n",
    "\n",
    "            # Datasets\n",
    "            res = prepare_datasets_for_ticker(dft, lookback_window, forecast_horizon)\n",
    "            if res[0] is None:\n",
    "                print(\"⚠️ Could not build datasets with required date windows.\"); continue\n",
    "            train_ds, val_ds, test_ds, scaler_x, _, test_dates, feat_cols, y_test_returns = res\n",
    "\n",
    "            train_loader = DataLoader(train_ds, **loader_kwargs_train)\n",
    "            val_loader   = DataLoader(val_ds,   **loader_kwargs_eval)\n",
    "            test_loader  = DataLoader(test_ds,  **loader_kwargs_eval)\n",
    "\n",
    "            # Modelo + trainer\n",
    "            model = TFTRegressor(input_size=len(feat_cols), hidden_size=hidden_size,\n",
    "                                 num_layers=num_layers, num_heads=num_heads, dropout=dropout)\n",
    "            trainer = Trainer(model, device=device, autocast_enabled=autocast_enabled)\n",
    "            trainer.fit(train_loader, val_loader, epochs=epochs, lr=3e-4)\n",
    "\n",
    "            # Carga mejor checkpoint y predice\n",
    "            model.load_state_dict(torch.load(\"best_tft_return.pth\", map_location=device))\n",
    "            preds, targets = trainer.predict(test_loader)\n",
    "\n",
    "            # Outputs\n",
    "            plot_return_curve(test_dates, preds, targets, t)\n",
    "            plot_losses(trainer, t)\n",
    "            acc, prec, rec, f1, rmse = evaluate_direction_from_returns(preds, targets)\n",
    "\n",
    "            accuracies.append(acc); precisions.append(prec); recalls.append(rec)\n",
    "            f1_scores.append(f1); RMSES.append(rmse); tick_list.append(t)\n",
    "\n",
    "            processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {t}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Guardado de métricas por ticker\n",
    "    if processed > 0:\n",
    "        results_df = pd.DataFrame({\n",
    "            'Ticker': tick_list,\n",
    "            'Accuracy': accuracies,\n",
    "            'Precision': precisions,\n",
    "            'Recall': recalls,\n",
    "            'F1-Score': f1_scores,\n",
    "            'RMSE': RMSES\n",
    "        })\n",
    "        print(\"\\nResultados por ticker:\")\n",
    "        print(results_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "        results_df.to_csv(\"tft_return_evaluation_results.csv\", index=False)\n",
    "        print(\"\\n💾 Saved -> tft_return_evaluation_results.csv\")\n",
    "    else:\n",
    "        print(\"❌ No se completó ningún ticker.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-08-20T11:56:29.507301Z",
     "iopub.status.idle": "2025-08-20T11:56:29.507519Z",
     "shell.execute_reply": "2025-08-20T11:56:29.507429Z",
     "shell.execute_reply.started": "2025-08-20T11:56:29.507419Z"
    },
    "id": "R25PAqoWYDmg"
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(\"tft_evaluation_results_sc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7901938,
     "sourceId": 12518480,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
